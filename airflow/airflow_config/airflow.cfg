[core]
# The home folder for airflow, default is ~/airflow
airflow_home = /opt/airflow

# The SQLAlchemy connection string to the metadata database
sql_alchemy_conn = sqlite:////opt/airflow/airflow.db

# The executor to use (SequentialExecutor, LocalExecutor, CeleryExecutor, etc.)
executor = LocalExecutor

# Whether to run DAGs in parallel
dag_concurrency = 16

# Number of task instances to run simultaneously in a single DAG
max_active_runs_per_dag = 16

# How long to wait before retrying failed tasks
retry_delay = 5 minutes

# How many retries before marking a task as failed
max_retry_delay = 30 minutes

# Default timezone to use for DAGs and tasks
default_timezone = utc

# Email configurations for sending task failure notifications
smtp_host = smtp.example.com
smtp_starttls = True
smtp_ssl = False
smtp_user = airflow@example.com
smtp_password = airflow_password
smtp_port = 587
smtp_mail_from = airflow@example.com

# Set to true to send notifications on task failure
email_on_failure = True
email_on_retry = False

[webserver]
# The base url of the Airflow web interface
base_url = http://localhost:8080

# Enable or disable webserver login (optional)
authenticate = False

# The port on which to run the web server
web_server_port = 8080

[logging]
# Log level for the Airflow components
logging_level = INFO

# Location of the log files for tasks and DAGs
base_log_folder = /opt/airflow/logs

# Whether to log web server requests
log_webserver = True

[scheduler]
# The number of task instances to run at a time
max_threads = 2

# How long to wait before terminating the scheduler
scheduler_task_queued_timeout = 600

# If true, the scheduler will run until all tasks are completed
catchup_by_default = False
